{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2317c2-128e-4072-a8c4-43d194f7c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c688f532-6888-4560-ab47-596f5f8fa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet은 깊은 신경망을 학습하기 위해 개발된 모델\n",
    "#잔차 학습 개념을 도입하여 매우 깊은 네트워크에서도 효율적인 학습이 가능하도록 함.\n",
    "#딥러닝 모델이 너무 깊어질 때 발생하는 기울기 소실 문제를 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36e792a3-1152-4fcd-82af-6d03bdee8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet의 기본 개념\n",
    "#1. 깊은 신경망의 문제\n",
    "# - 깊은 신경망은 더 많은 계층을 쌓아 복잡한 패턴을 학습할 수 있지만, 너무 깊어지면 학습이 어려워지는 문제.\n",
    "# - 주로 기울기 소실이나 기울기 폭발같은 현상 떄문에 발생\n",
    "# - 이는 모델이 더 이상 깊어지지 못하고 성능이 저하되는 결과를 초래\n",
    "\n",
    "#2. 잔차 학습\n",
    "# - ResNet은 이런 문제를 해결하기 위해 잔차 학습을 도입\n",
    "# - 잔차 학습은 각 층의 출력이 바로 다음 층의 입력으로 전달되지 않고, 이전 층의 입력을 더해줌으로써 학습을 도움.\n",
    "# - 이를 통해 기울기 소실 문제를 완화할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8838473b-6498-46ff-b3d5-e6bb7c1d2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet의 주요 특징\n",
    "\n",
    "# 기울기 소실 문제 해결\n",
    "# - 잔차 학습을 통해 깊은 네트워크에서도 기울기 소실 문제를 해결\n",
    "# - 입력을 출력에 더해줌으로써 신호가 더욱 쉽게 전달되어 학습이 원활하게 이루어짐.\n",
    "\n",
    "# 간단한 블록 구조\n",
    "# - ResNet은 간단한 블록 구조를 사용하여 네트워크를 쉽게 확장할 수 있음.\n",
    "\n",
    "# 높은 성능\n",
    "# - ResNet은 이미지 분류, 객체 검출 등 다양한 컴퓨터 비전 작업에서 높은 성능을 발휘\n",
    "# - 깊은 네트워크에서도 안정적으로 학습할 수 있어, 복잡한 패턴을 잘 학습함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae5379-4925-4eb2-a903-58c5589107a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075cb876-2e6d-4e41-a99b-f1cfaf664787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet 실습\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        # 첫 번째 컨볼루션 레이어\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)  # 배치 정규화\n",
    "        # 두 번째 컨볼루션 레이어\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)  # 배치 정규화\n",
    "\n",
    "        # 입력과 출력의 차원이 다를 경우 shortcut 경로 정의\n",
    "        self.skip_connection = nn.Sequential()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),  # 차원 맞추기 위한 1x1 컨볼루션\n",
    "                nn.BatchNorm2d(out_ch)  # 배치 정규화\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 첫 번째 컨볼루션 + ReLU 활성화 함수\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 두 번째 컨볼루션 후 배치 정규화\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        # shortcut 경로 출력과 현재 블록의 출력 더하기\n",
    "        output += self.skip_connection(x)\n",
    "        # 최종 ReLU 활성화 함수 적용\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "\n",
    "# ResNet 모델 정의\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.initial_channels = 64  # 첫 번째 레이어의 입력 채널 수 정의\n",
    "        # 첫 번째 컨볼루션 레이어\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # 배치 정규화\n",
    "        # ResNet의 각 레이어 생성\n",
    "        self.layer1 = self._create_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._create_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._create_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._create_layer(block, 512, layers[3], stride=2)\n",
    "        # 평균 풀링 레이어\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # 최종 완전 연결 레이어\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    # ResNet의 각 레이어를 생성하는 함수\n",
    "    def _create_layer(self, block, out_ch, num_layers, stride):\n",
    "        layer_list = []\n",
    "        # 첫 번째 블록은 stride를 받을 수 있음\n",
    "        layer_list.append(block(self.initial_channels, out_ch, stride))\n",
    "        self.initial_channels = out_ch  # 다음 블록을 위해 채널 수 업데이트\n",
    "        # 나머지 블록들은 기본 stride를 사용\n",
    "        for _ in range(1, num_layers):\n",
    "            layer_list.append(block(out_ch, out_ch))\n",
    "        return nn.Sequential(*layer_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 첫 번째 컨볼루션 + ReLU 활성화 함수\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        # 각 레이어를 순차적으로 통과\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # 평균 풀링 및 텐서의 차원 축소\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # 최종 완전 연결 레이어를 통해 클래스별 예측값 출력\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Custom ResNet-18 모델 생성 (각 레이어의 블록 수는 2개씩)\n",
    "model = CustomResNet(Block, [2, 2, 2, 2], num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbd009-fe00-4391-ac2d-9f879ab87b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sparta)",
   "language": "python",
   "name": "sparta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

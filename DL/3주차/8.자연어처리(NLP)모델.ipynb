{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5298c81-9251-4021-93a4-ee35604dbcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어 처리(NLP) 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a48281e-e997-4d42-8f5c-b7acf7dcfbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#워드 임베딩 기법\n",
    "#워드 임베딩(Word Embedding)은 단어를 고정된 크기의 벡터로 변환하는 기법\n",
    "#단어 간의 의미적 유사성을 반영\n",
    "\n",
    "#Word2Vec과 GloVe가 있음\n",
    "\n",
    "#예\n",
    "# 남자 -> 여자\n",
    "# 아빠 -> 엄마\n",
    "# 아들 -> 딸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cb68a8-9091-4be0-8bba-74b6fccee7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec\n",
    "#단어를 벡터로 변환하는 두 가지 모델(CBOW와 Skip-gram)을 제공\n",
    "\n",
    "#CBOW(Continuous Bag of Words): 주변 단어(context)로 중심 단어(target)를 예측\n",
    "#Skip-gram: 중심 단어(target)로 주변 단어(context)를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2892ef10-d042-473b-be88-24dbb3df12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Glove(Global Vectors for Word Representation)\n",
    "#Glove는 단어-단어 공기행렬(word-word-co-occurence matrix)을 사용, 단어 벡터를 학습\n",
    "#전역적인 통계 정보를 활용하여 단어 간의 의미적 유사성을 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacaac15-74b2-4c62-8410-95389e70edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca8e861-3c58-408b-bff8-bc79bab1a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시퀀스 모델링의 기본 개념\n",
    "#시퀀스 모델링은 순차적인 데이터를 처리하고 예측하는 모델링 기법\n",
    "#시퀀스 모델링은 주로 RNN, LSTM, GRU와 같은 순환 신경망을 사용\n",
    "\n",
    "#입력 시퀀스 -> 은닉상태(반복) -> 출력 시퀀스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231aca7a-53fb-4011-9f3b-82b919d04b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력 시퀀스\n",
    "# 시퀀스 모델링에서는 입력 데이터가 순차적인 형태로 제공\n",
    "#예를 들어, 텍스트 데이터는 단어의 시퀀스로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c86544d-02a7-48e9-bb8d-a529c140b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#은닉 상태\n",
    "#순환 신경망은 이전 시간 단계의 은닉 상태를 현재 시간 단계로 전달하여, 시퀀스의 패턴을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6efeecb5-1abc-4e40-90db-7b9b6c845936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#출력 시퀀스\n",
    "#시퀀스 모델링의 출력은 입력 시퀀스와 동일한 길이의 시퀀스일 수도 있고, 단일 값일 수도 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61123e7-a684-4eb2-85af-d656f0248b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b0d833-4baf-4034-8640-e64e3bf18d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer와 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da01b784-1e2f-4069-b5c9-044fe864b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer의 구조와 원리\n",
    "#Transformer는 순차적인 데이터를 병렬로 처리할 수 있는 모델로, 자연어 처리에서 뛰어난 성능을 보임.\n",
    "#Transformer는 인코더-디코더(Encoder-Decoder) 구조로 구성\n",
    "\n",
    "#예\n",
    "#1. 워드 임베딩 - '내 고양이는 노랗다'\n",
    "#2. 인코더(셀프 어텐션, 피드 포워드 신경망)\n",
    "#3. 디코더(셀프 어텐션, 인코더-디코더 어텐션, 피드 포워드 신경망\n",
    "#4. My cat is yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7be9e6-bdbf-43ca-bfe5-f807bf3782bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더\n",
    "#입력 시퀀스를 처리하여 인코딩된 표현을 생성\n",
    "#각 인코더 층은 셀프 어텐션과 피드 포워드 신경망으로 구성1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db0f23b1-f7bb-4b51-b55c-488bf7ba2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#디코더\n",
    "#인코딩된 표현을 바탕으로 출력 시퀀스를 생성\n",
    "#각 디코더 층은 셀프 어텐션, 인코더-디코더 어텐션, 피드 포워드 신경망으로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5715d8-e252-4897-925c-01816f34668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#어텐션 매커니즘\n",
    "#어텐션 매커니즘은 입력 시퀀스의 각 위치에 가중치를 부여하여, 중요한 정보를 강조\n",
    "#셀프 어텐션은 입력 시퀀스 내의 단어 간의 관계를 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d120ed-64df-44b5-a056-4eb7cf969a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a1e095-a701-4d3f-b637-f69dca0deb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT(Bidirectional Encoder Representations from Transformers)\n",
    "#인코더를 기반으로 한 사전 학습된 언어 모델\n",
    "#양방향으로 문맥을 이해할 수 있어, 다양한 자연어 처리 작업에서 뛰어난 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7675cb7-daff-46fd-b141-69733ab3569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전 학습(Pre-training)\n",
    "#BERT는 대규모 텍스트 코퍼스를 사용하여 사전 학습됨.\n",
    "#마스킹 언어 모델과 다음 문장 예측 작업을 통해 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d4f54-92fa-4c78-a4e3-4cf2e3d15203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파인 튜닝(Fine-tuning)\n",
    "#사전 학습된 BERT 모델을 특정 작업에 맞게 파인 튜닝\n",
    "#텍스트 분류, 질의 응답, 텍스트 생성 등 다양한 자연어 처리 작업에 적용할 수 있음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_learn)",
   "language": "python",
   "name": "dl_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

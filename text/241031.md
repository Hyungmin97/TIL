# 24년 10월 31일 TIL
### AI 모델 활용
--- 
- **API(Application Programming Interface)**   
: 개발자가 복잡한 AI 기능을 손쉽게 사용할 수 있도록 제공되는 인터페이스이며, AI 알고리즘을 직접 개발할 필요 없이, AI 기능을 이용할 수 있음.
<br>

- **사전 학습 모델**   
: 많은 데이터로 미리 학습 시킨 AI 모델이며, 학습 과정을 생략하고, 곧바로 예측이나 분류 작업에 사용할 수 있음.
---

- **패키지**   
: 여러 모듈들을 묶어놓은 하나의 디렉토리   
예) 데이터 분석: **panda**, 수학 계산: **numpy**, 웹 개발: **Django** 등   
>**pip 패키지**   
: pip은 파이썬 패키지 관리자. 이를 통해 패키지를 쉽게 설치 및 관리 가능   
>> 설치 방법1)   
**pip install 패키지명**   
>> 설치 방법2)   
**pip install 패키지명==버전번호**   
>>> pip 패키지 목록 확인   
>>> **pip list**   

- 같은 패키지라도 버전에 따라 기능이 달라질 수 있기 때문에 버전 관리가 중요함. 
---
- **가상환경 (Conda, Venv)**   
: 프로젝트마다 독립된 환경을 구성해주는 도구이며, 가상환경을 통해 다른 프로젝트와 패키지 간 충돌을 방지할 수 있음.
<br>

> **conda**   
: 패키지와 환경을 동시에 관리할 수 있는 도구로, 데이터 과학 분야에서 많이 사용됨.   
>> 1. 가상환경 만들기   
**conda create --name 환경이름**   
>> 2. 가상환경 활성화   
**conda activate 환경이름**   
>> 3. 가상환경 비활성화   
**conda deactivate**   

>>**추가 확인 사항**   
>> 4. [가상환경 삭제] 커널 목록 확인:   
**jupyter kernelspec list**   
>> 5. [가상환경 삭제] 커널 제거   
**jupyter kernelspec remove <커널_이름>**   
<br>   

- Jupyter Notebook에서 가상환경 설정하기   
> 1. ipykernel 설치   
**conda install -c anaconda ipykernel**   
> 2. 커널 등록   
**python -m ipykernel install --user --name 환경이름 --display-name "가상환경 이름"**   
<br>

- venv로 가상환경 설정하기   
: venv는 파이썬 표준 라이브러리에서 제공하는 가상환경 도구   
> 1. 가상환경 만들기   
**python -m venv 환경이름**   
> 2. 가상환경 활성화   
>> Windows   
**환경이름\\Scripts\\activate**   
>> Mac/Linux   
**source 환경이름/bin/activate**   
> 3. 가상환경 비활성화   
**deactivate**   
<br>

- requirements.txt 사용법   
: 프로젝트에서 사용된 모든 패키지와 그 버전을 기록해놓은 파일.   
-> 다른 개발자가 같은 환경을 구성할 때 사용하면 좋음.   
> 1. requirements.txt 파일 생성하기   
**pip freeze > requirements.txt**   
> 2. requirements.txt 파일로 패키지 설치하기   
**pip install -r requirements.txt**   
--- 
- 허깅페이스(Hugging Face)   
: 자연어 처리(NLP)를 중심으로 다양한 AI 모델들을 제공하는 플랫폼   
<br>

> 특징   
> 1. Transformers 라이브러리   
: BERT, GPT-3와 같은 최신 NLP 모델을 쉽게 사용할 수 있음.   
> 2. 모델 허브(Model Hub)   
: 수천 개의 미리 학습된 모델들이 모여있는 곳이며, 허브에서 모델들을 가져다쓸 수 있음.   
> 3. 커뮤니티 중심   
: 오픈소스 커뮤니티를 중심으로 운영   

> 허깅페이스의 장점/단점   
> 장점   
> 1. 쉬운 접근성   
> 2. 광범위한 모델 선택   
> 3. 오픈 소스   
> 4. 강력한 커뮤니티 지원   

> **단점**   
> 1. 리소스 요구량   
: 고성능 모델을 사용하려면 강력한 컴퓨팅 자원 필요   
> 2. 복잡한 초기 설정   
> 3. 특화된 모델   
: NLP 외의 다른 AI 분야에서는 상대적으로 모델의 수가 적음.   
<br>

- 허깅페이스 활용   
: 텍스트 생성, 감정 분석, 번역 등 다양한 NLP 작업을 쉽게 처리할 수 있음. 또한, 모델을 훈련시키거나 미세 조정하여 자신만의 AI 모델을 만들 수 있,요즘은 NLP를 넘어 컴퓨터 비전(CV), 강화 학습(RL) 등 다양한 분야로 확장 중.   
---

- **API**    
: 서로 다른 소프트웨어가 "대화"할 수 있도록 돕는 일종의 통로이며, 서버와 클라이언트 간에 요청과 응답을 주고받는 방식으로 작동   

> API의 장점/단점   
> 장점   
> 1. 손쉬운 사용   
> 2. 신속한 개발   
> 3. 확장성   

> **단점**   
> 1. 비용   
: 고성능 모델을 사용하려면 강력한 컴퓨팅 자원 필요   
> 2. 제한된 제어   
: 제공된 기능만 사용할 수 있으며, 커스터마이징 제한적   
> 3. 의존성   
: 해당 API가 서비스를 종료하거나 변경될 시, 문제 발생 가능성.   
<br>

### PyTorch를 사용해 Transformer 모델 구현   
- Transformer   
: 자연어 처리(NLP)에서 뛰어난 성능을 보이는 모델.   
-> Self-Attention 매커니즘을 활용해 텍스트의 문맥을 파악하고, 병렬 처리에 강한 구조.   
-> BERT, GPT, T5 같은 모델들이 모두 Transformer 기반   

> 1. PyTorch 설치 및 설정   
>> **pip install torch torchvision torchaudio**   
> 2. Transformer 모델 불러오기   
>> **model = Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6)**   
> 3. 모델 학습 준비, 학습 시키기   
>> 모델 학습 준비    
**optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)**   
**criterion = nn.CrossEntropyLoss()**   
>> 모델 학습   
**for epoch in range(num_epochs):**   
    &ensp;&ensp;&ensp;&ensp;**optimizer.&ensp;&ensp;zero_grad()**   
    &ensp;&ensp;&ensp;&ensp;**output = model(src, tgt)**   
    &ensp;&ensp;&ensp;&ensp;**loss = criterion(output, tgt_labels)**   
    &ensp;&ensp;&ensp;&ensp;**loss.backward()**   
    &ensp;&ensp;&ensp;&ensp;**optimizer.step()**   
<br>

### 사전 학습된 모델 활용하기   
: PyTorch 허브를 이용하면, 사전 학습된 다양한 모델들을 사용할 수 있음.   
예) torch.hub.load()를 사용해 GPT-2 같은 사전 학습된 모델을 불러올 수 있음.   
-> 불러온 모델은 이미 방대한 데이터셋으로 학습된 상태이기 때문에 바로 활용 가능.   

> **Transformer의 한계점**   
> 1. 대형 모델 학습 어려움.   
> 2. 복잡한 모델은 직접 만들기 어려움.   
> 3. 사전 학습된 모델 활용의 한계   
--- 

### 사전 학습(Pre-training)   
: 대규모 텍스트 데이터셋을 사용해 모델이 일반적인 언어 이해 능력을 학습하는 과정   
-> 특정 작업(예: 번역, 감정 분석 등)을 염두에 두지 않고, 단순히 언어의 패턴과 구조를 학습하는 것이 목적   

> - 특징   
> 1. 대규모 데이터셋 사용   
: 인터넷에서 수집한 방대한 양의 텍스트 데이터로 모델을 학습시킴.   
예) BERT는 수십억 개의 문장으로 사전 학습시킴.   
> 2. 일반적인 언어 이행   
: 모델은 텍스트 내 단어의 의미, 문장 구조, 문잭 등 언어의 전반적인 특징 학습.   
> 3. 작업 비특화   
: 특정 작업에 맞춰진 학습이 아닌, 전반적인 언어 이해에 초점을 맞춤.   

- 사전 학습의 목적   
: 다양한 텍스트에서 언어의 기본적인 규칙을 배우고, 이후 특정 작업에 빠르게 적응할 수 있는 기반을 다짐.   


**예시: BERT의 사전 학습**   
1. Masked Language Modeling (MLM)   
: 문장의 일부 단어를 마스킹한 후, 이를 예측하도록 모델을 학습시킴. 이를 통해 BERT는 문맥을 양방향으로 이해할 수 있음.   
1. Next Sentence Prediction (NSP)   
: 두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장 뒤에 자연스럽게 이어지는 지를 예측. 이를 통해 문장 간의 관계를 이해하는 능력을 학습.   
--- 
### 파인 튜닝(Fine-Tuning)   
: 사전 학습된 모델을 특정 작업에 맞게 가로 학습시키는 과정.
예) BERT 모델을 감정 분석에 사용하려면, BERT의 사전 학습된 가중치를 유지하면서 감정 분석 작업에 맞게 모델의 가중치를 조절    

> - 파인 튜닝의 특징   
> 1. 작업 특화   
: 특정 작업(예: 텍스트 분류, 번역, 질의 응답 등)에 맞춰 모델을 최적화   
> 2. 사전 학습 가중치 활용   
: 사전 학습된 모델의 언어 이해 능력을 바탕으로, 새로운 작업에 적응할 수 있도록 일부 가중치만 조정   
> 3. 적은 데이터로도 가능   
: 사전 학습으로 인하여, 비교적 적은 양의 데이터로도 효과적인 학습 가능   

- 파인 튜닝의 목적   
: 특정 작업에서 최상의 성능을 발휘하도록 모델을 조정하는 과정.   
-> 사전 학습으로 인하여 파인 튜닝은 더 빠르고 적은 데이터로 이루어질 수 있음.   
-> 대부분의 NLP 모델은 파인 튜닝 과정을 거쳐 실제 어플리케이션에서 사용.   

